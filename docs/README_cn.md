# LLM training demo

æœ¬é¡¹ç›®ä¸ºä»é›¶è®­ç»ƒGPTé£æ ¼å¤§æ¨¡å‹çš„æç®€å®ç”¨ä»£ç ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€SFTã€RLHF/DPOç­‰æµç¨‹ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
conda create -n llm python=3.10 -y
conda activate llm
pip install -r requirements.txt
```

### 2. ä¸‹è½½æ•°æ®

```bash
python src/data/download_base.py --dataset wikitext2
python src/data/download_sft.py --dataset alpaca
python src/data/download_preference.py --dataset ultrafeedback
```

### 3. è®­ç»ƒåˆ†è¯å™¨

```bash
python src/data/tokenizer.py --action train
```

### 4. é¢„è®­ç»ƒGPTæ¨¡å‹

```bash
python src/training/pretrain.py --model_size small --batch_size 8 --num_epochs 10
```

### 5. SFTæŒ‡ä»¤å¾®è°ƒ

```bash
python src/training/sft.py --pretrained_path pretrain_run --prompt_template alpaca --batch_size 4 --num_epochs 3
```

### 6. DPOåå¥½è®­ç»ƒ

```bash
python src/training/dpo.py --sft_model_path sft_run --batch_size 4 --num_epochs 1
```

### 7. RLHFè®­ç»ƒï¼ˆå¯é€‰ï¼‰

```bash
python src/training/rlhf.py reward --sft_model_path sft_run
python src/training/rlhf.py ppo --sft_model_path sft_run --reward_model_path reward_model_run
```

---
