"""
Download SFT instruction fine-tuning datasets
"""

import json
from pathlib import Path
from datasets import load_dataset


DATA_DIR = Path(__file__).parent.parent.parent / "data"


def download_alpaca(save_dir: Path = None, num_samples: int = None):
    """
    Download Alpaca dataset
    - 52K instruction data
    - Generated by Stanford using GPT-3.5
    - Classic SFT starter dataset
    """
    save_dir = save_dir or DATA_DIR / "sft"
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print("Downloading Alpaca dataset...")
    dataset = load_dataset("tatsu-lab/alpaca", split="train")
    
    if num_samples:
        dataset = dataset.select(range(min(num_samples, len(dataset))))
    
    # Convert to JSONL format
    train_data = []
    for item in dataset:
        train_data.append({
            "instruction": item["instruction"],
            "input": item["input"],
            "output": item["output"],
        })
    
    # Split into train and validation sets (95% / 5%)
    split_idx = int(len(train_data) * 0.95)
    train_split = train_data[:split_idx]
    val_split = train_data[split_idx:]
    
    # Save
    with open(save_dir / "train.jsonl", "w", encoding="utf-8") as f:
        for item in train_split:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    with open(save_dir / "val.jsonl", "w", encoding="utf-8") as f:
        for item in val_split:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"Alpaca dataset download complete!")
    print(f"  Train set: {save_dir / 'train.jsonl'} ({len(train_split)} samples)")
    print(f"  Val set: {save_dir / 'val.jsonl'} ({len(val_split)} samples)")


def download_dolly(save_dir: Path = None):
    """
    Download Dolly dataset
    - 15K high-quality human-annotated data
    - Annotated by Databricks employees
    - High quality
    """
    save_dir = save_dir or DATA_DIR / "sft"
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print("Downloading Dolly dataset...")
    dataset = load_dataset("databricks/databricks-dolly-15k", split="train")
    
    # Convert format
    train_data = []
    for item in dataset:
        train_data.append({
            "instruction": item["instruction"],
            "input": item["context"],
            "output": item["response"],
        })
    
    # Split
    split_idx = int(len(train_data) * 0.95)
    train_split = train_data[:split_idx]
    val_split = train_data[split_idx:]
    
    # Save
    with open(save_dir / "train.jsonl", "w", encoding="utf-8") as f:
        for item in train_split:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    with open(save_dir / "val.jsonl", "w", encoding="utf-8") as f:
        for item in val_split:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"Dolly dataset download complete!")
    print(f"  Train set: {save_dir / 'train.jsonl'} ({len(train_split)} samples)")
    print(f"  Val set: {save_dir / 'val.jsonl'} ({len(val_split)} samples)")


def download_oasst(save_dir: Path = None, num_samples: int = 10000):
    """
    Download OpenAssistant dataset
    - Large-scale multilingual human conversation data
    - Includes multi-turn dialogues
    """
    save_dir = save_dir or DATA_DIR / "sft"
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print("Downloading OpenAssistant dataset...")
    dataset = load_dataset("OpenAssistant/oasst1", split="train")
    
    # Filter English prompter-assistant pairs
    # Build message tree
    messages_by_parent = {}
    messages_by_id = {}
    
    for item in dataset:
        msg_id = item["message_id"]
        parent_id = item["parent_id"]
        messages_by_id[msg_id] = item
        
        if parent_id:
            if parent_id not in messages_by_parent:
                messages_by_parent[parent_id] = []
            messages_by_parent[parent_id].append(item)
    
    # Extract instruction-response pairs
    train_data = []
    for item in dataset:
        if item["role"] == "prompter" and item["lang"] == "en":
            # Find corresponding assistant reply
            if item["message_id"] in messages_by_parent:
                replies = messages_by_parent[item["message_id"]]
                # Select the highest-rated reply
                assistant_replies = [r for r in replies if r["role"] == "assistant"]
                if assistant_replies:
                    best_reply = max(assistant_replies, key=lambda x: x.get("rank", 0) or 0)
                    train_data.append({
                        "instruction": item["text"],
                        "input": "",
                        "output": best_reply["text"],
                    })
        
        if len(train_data) >= num_samples:
            break
    
    # Split
    split_idx = int(len(train_data) * 0.95)
    train_split = train_data[:split_idx]
    val_split = train_data[split_idx:]
    
    # Save
    with open(save_dir / "train.jsonl", "w", encoding="utf-8") as f:
        for item in train_split:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    with open(save_dir / "val.jsonl", "w", encoding="utf-8") as f:
        for item in val_split:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"OpenAssistant dataset download complete!")
    print(f"  Train set: {save_dir / 'train.jsonl'} ({len(train_split)} samples)")
    print(f"  Val set: {save_dir / 'val.jsonl'} ({len(val_split)} samples)")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Download SFT datasets")
    parser.add_argument(
        "--dataset",
        type=str,
        default="alpaca",
        choices=["alpaca", "dolly", "oasst"],
        help="Select dataset: alpaca (52K), dolly (15K), oasst (configurable)"
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        default=None,
        help="Number of samples to download (only valid for alpaca and oasst)"
    )
    
    args = parser.parse_args()
    
    if args.dataset == "alpaca":
        download_alpaca(num_samples=args.num_samples)
    elif args.dataset == "dolly":
        download_dolly()
    elif args.dataset == "oasst":
        download_oasst(num_samples=args.num_samples or 10000)